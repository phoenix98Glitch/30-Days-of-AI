{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello World üòÉ! This is my first kaggle notebook, where I will be implementing Fully Convolutional Networks for semantic segmentation on images using `Keras`. For building our model, we have used the `cityscapes-image-pairs` dataset, containing 2975 images which are used for training and 500 images for validating the performance of our model.","metadata":{}},{"cell_type":"markdown","source":"![FCN](https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-10.34.02-PM.png)\n\nImage segmentation can be interpreted as a classification problem, where the task is to classify each pixel of the image into a particular class. To build an end-to-end pixel-to-pixel segmentation network, our model must be capable of extracting rich spatial information from the images. A typical CNN used for classification takes an image as input, passes it through a series of convolutional and pooling layers and uses fully-connected layers in the end to output a fixed length vector, thus discarding all the spatial information from the original image. However, if the fully-connected layers at the end are replaced by convolutional layers, we get coarse spatial features as output instead of vectors, which can be further upsampled to form the classification maps corresponding to our image.\n\n![Segmentation Architecture](https://csdl-images.computer.org/trans/tp/2017/04/figures/shelh3-2572683.gif)\n\nThe architecture used in this notebook is shown in the figure above. The segmentation operation performed by the network can be divided into two parts -\n\n* **Downsampling operation -** This part is simple. Our image is propagated through a series of convolutional and pooling layers to extract the spatial information. For this I have used the `VGG-16` architecture initialized with weights pretrained on the Imagenet dataset. The final fully connected layers of the network are discarded and two new convolutional layers `conv6` and `conv7` are added in their place. While training, the entire model is trained, thus fine-tuning the pretrained weights on our dataset.\n\n* **Upsampling operation -** This generates the classification map from the feature map produced in the Downsampling operation. The operation is also referred as deconvolution or fractionally strided convolution operation, i.e. features of smaller spatial resolution are mapped to larger spatial resolution. Depending upon the upsampling strategy, our network can be of three types as shown in the figure -\n\n    * **FCN-32s** - Directly produces segmentation map from `conv7` layer by using transpose convolution with stride 32\n    * **FCN-16s** - Add 1 x 1 convolution on `pool4` and fuse it with 2X upsampled `conv7`. The segmentation map is then produced by using a transpose convolution on the result with stride 16 \n    * **FCN-8s** - Add 1 x 1 convolution on `pool3` and fuse it with 2X upsampled fused predictions of `conv7` and `pool4`. The segmentation map is then produced by using a transpose convolution on the result with stride 8","metadata":{}},{"cell_type":"markdown","source":"So let's dive into the code and see how the above model can be implemented üòÅ.","metadata":{}},{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plot\nfrom PIL import Image\nimport cv2\nimport random\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-04T16:50:37.639644Z","iopub.execute_input":"2021-11-04T16:50:37.640350Z","iopub.status.idle":"2021-11-04T16:50:37.646227Z","shell.execute_reply.started":"2021-11-04T16:50:37.640268Z","shell.execute_reply":"2021-11-04T16:50:37.645051Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Activation, Add, Conv2DTranspose\nfrom keras.applications.vgg16 import VGG16","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:37.648214Z","iopub.execute_input":"2021-11-04T16:50:37.648844Z","iopub.status.idle":"2021-11-04T16:50:37.657049Z","shell.execute_reply.started":"2021-11-04T16:50:37.648792Z","shell.execute_reply":"2021-11-04T16:50:37.656039Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Variables","metadata":{}},{"cell_type":"markdown","source":"After importing the libraries, we initialize all the necessary variables - \n* `train_folder` - Path for training images\n* `valid_folder` - Path for testing images\n* `width` - Width of an image\n* `height` - Height of an image\n* `classes` - No. of discrete pixel values in the segmentation maps (no. of labels)\n* `batch_size` - Size of a single batch\n* `num_of_training_samples` - Total number of training samples\n* `num_of_testing_samples` - Total number of testing samples","metadata":{}},{"cell_type":"code","source":"train_folder=\"/kaggle/input/cityscapes-image-pairs/cityscapes_data/cityscapes_data/train\"\nvalid_folder=\"/kaggle/input/cityscapes-image-pairs/cityscapes_data/cityscapes_data/val\"\nwidth = 256\nheight = 256\nclasses = 13\nbatch_size = 10\nnum_of_training_samples = len(os.listdir(train_folder)) \nnum_of_testing_samples = len(os.listdir(valid_folder))","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:37.658787Z","iopub.execute_input":"2021-11-04T16:50:37.659467Z","iopub.status.idle":"2021-11-04T16:50:38.042997Z","shell.execute_reply.started":"2021-11-04T16:50:37.659418Z","shell.execute_reply":"2021-11-04T16:50:38.042211Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"For preprocessing the dataset and defining the model, we have defined several helper functions -\n\n* `LoadImage` - Loads a single image and its corresponding segmentation map \n    * **Arguements** :\n        * `name` - Name of the image file\n        * `path` - Path to the image directory\n    * **Returns** - A tuple of 2 numpy arrays (image and segmentation map)\n    \n    \n* `bin_image` - Bin a segmentation map (Converts pixels from range (0, 255) to (0, classes))\n    * **Arguements** :\n        * `mask` - Original segmentation map\n    * **Returns** - New segmentation mask after binning pixel values\n    \n    \n* `getSegmentationArr` - Convert RGB segmentation maps to categorical maps used for training our model\n    * **Arguements** :\n        * `image` - Segmentation mask after binning\n        * `classes` - Number of categories or unique pixel values (13)\n        * `width` - Width of segmentation map\n        * `height` - Height of segmentation map\n    * **Returns** - Categorical segmentation map (width, height, classes)\n    \n    \n* `give_color_to_seg_img` - Convert categorical arrays back to colored segmentation maps\n    * **Arguements** : \n        * `seg` - Categorical segmentation map (width, height, classes)\n        * `n_classes` - Number of categories or unique pixel values (13) \n    * **Returns** - Colored segmentation map (width, height, 3)\n    \n    \n* `DataGenerator` - Returns data in form of batches\n    * **Arguements** :\n        * `path` - location or path of the image directory\n        * `batch_size` - size of each batch\n        * `classes` - Number of categories or unique pixel values (13)\n    * **Returns** - Tuple of `batch_size` number of images and segmentation maps\n    \n    \n* `fcn` - Creates the FCN model\n    * **Arguements** :\n        * `vgg` - VGG16 pretrained model\n        * `classes` - Number of categories or unique pixel values (13)\n        * `fcn8` - Set True to use FCN-8s model\n        * `fcn16` - Set True to use FCN-16s model\n    * **Returns** - FCN model\n    * **Note** - If both `fcn8` and `fcn16` arguements are set to False, it returns FCN-32s model by default","metadata":{}},{"cell_type":"markdown","source":"## Load Image and Segmentation Mask","metadata":{}},{"cell_type":"code","source":"def LoadImage(name, path):\n    img = Image.open(os.path.join(path, name))\n    img = np.array(img)\n    \n    image = img[:,:256]\n    mask = img[:,256:]\n    \n    return image, mask","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.044993Z","iopub.execute_input":"2021-11-04T16:50:38.045551Z","iopub.status.idle":"2021-11-04T16:50:38.052626Z","shell.execute_reply.started":"2021-11-04T16:50:38.045357Z","shell.execute_reply":"2021-11-04T16:50:38.051348Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Bin Segmentation Mask ","metadata":{}},{"cell_type":"code","source":"def bin_image(mask):\n    bins = np.array([20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 220, 240])\n    new_mask = np.digitize(mask, bins)\n    return new_mask","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.056522Z","iopub.execute_input":"2021-11-04T16:50:38.056905Z","iopub.status.idle":"2021-11-04T16:50:38.065250Z","shell.execute_reply.started":"2021-11-04T16:50:38.056852Z","shell.execute_reply":"2021-11-04T16:50:38.064498Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Segmentation Masks to Categorical Arrays ","metadata":{}},{"cell_type":"code","source":"def getSegmentationArr(image, classes, width=width, height=height):\n    seg_labels = np.zeros((height, width, classes))\n    img = image[:, : , 0]\n\n    for c in range(classes):\n        seg_labels[:, :, c] = (img == c ).astype(int)\n    return seg_labels","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.069350Z","iopub.execute_input":"2021-11-04T16:50:38.069720Z","iopub.status.idle":"2021-11-04T16:50:38.076605Z","shell.execute_reply.started":"2021-11-04T16:50:38.069664Z","shell.execute_reply":"2021-11-04T16:50:38.075561Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Arrays to Colored Segmentation Masks","metadata":{}},{"cell_type":"code","source":"def give_color_to_seg_img(seg, n_classes=13):\n    \n    seg_img = np.zeros( (seg.shape[0],seg.shape[1],3) ).astype('float')\n    colors = sns.color_palette(\"hls\", n_classes)\n    \n    for c in range(n_classes):\n        segc = (seg == c)\n        seg_img[:,:,0] += (segc*( colors[c][0] ))\n        seg_img[:,:,1] += (segc*( colors[c][1] ))\n        seg_img[:,:,2] += (segc*( colors[c][2] ))\n\n    return(seg_img)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.078161Z","iopub.execute_input":"2021-11-04T16:50:38.078690Z","iopub.status.idle":"2021-11-04T16:50:38.088316Z","shell.execute_reply.started":"2021-11-04T16:50:38.078524Z","shell.execute_reply":"2021-11-04T16:50:38.087154Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Generator function to generate data batches","metadata":{}},{"cell_type":"code","source":"def DataGenerator(path, batch_size=10, classes=13):\n    files = os.listdir(path)\n    while True:\n        for i in range(0, len(files), batch_size):\n            batch_files = files[i : i+batch_size]\n            imgs=[]\n            segs=[]\n            for file in batch_files:\n                #file = random.sample(files,1)[0]\n                image, mask = LoadImage(file, path)\n                mask_binned = bin_image(mask)\n                labels = getSegmentationArr(mask_binned, classes)\n\n                imgs.append(image)\n                segs.append(labels)\n\n            yield np.array(imgs), np.array(segs)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.090159Z","iopub.execute_input":"2021-11-04T16:50:38.090780Z","iopub.status.idle":"2021-11-04T16:50:38.100239Z","shell.execute_reply.started":"2021-11-04T16:50:38.090620Z","shell.execute_reply":"2021-11-04T16:50:38.098930Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Data Samples","metadata":{}},{"cell_type":"code","source":"train_gen = DataGenerator(train_folder, batch_size=batch_size)\nval_gen = DataGenerator(valid_folder, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.101870Z","iopub.execute_input":"2021-11-04T16:50:38.102406Z","iopub.status.idle":"2021-11-04T16:50:38.108896Z","shell.execute_reply.started":"2021-11-04T16:50:38.102236Z","shell.execute_reply":"2021-11-04T16:50:38.107693Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"imgs, segs = next(train_gen)\nimgs.shape, segs.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.110562Z","iopub.execute_input":"2021-11-04T16:50:38.111219Z","iopub.status.idle":"2021-11-04T16:50:38.344065Z","shell.execute_reply.started":"2021-11-04T16:50:38.110910Z","shell.execute_reply":"2021-11-04T16:50:38.343166Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"image = imgs[0]\nmask = give_color_to_seg_img(np.argmax(segs[0], axis=-1))\nmasked_image = cv2.addWeighted(image/255, 0.5, mask, 0.5, 0)\n\nfig, axs = plot.subplots(1, 3, figsize=(20,20))\naxs[0].imshow(image)\naxs[0].set_title('Original Image')\naxs[1].imshow(mask)\naxs[1].set_title('Segmentation Mask')\n#predimg = cv2.addWeighted(imgs[i]/255, 0.6, _p, 0.4, 0)\naxs[2].imshow(masked_image)\naxs[2].set_title('Masked Image')\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:38.345579Z","iopub.execute_input":"2021-11-04T16:50:38.346078Z","iopub.status.idle":"2021-11-04T16:50:39.142533Z","shell.execute_reply.started":"2021-11-04T16:50:38.345909Z","shell.execute_reply":"2021-11-04T16:50:39.141032Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Segmentation model - FCN+Transfer Learning","metadata":{}},{"cell_type":"code","source":"def fcn(vgg, classes = 13, fcn8 = False, fcn16 = False):\n    pool5 = vgg.get_layer('block5_pool').output \n    pool4 = vgg.get_layer('block4_pool').output\n    pool3 = vgg.get_layer('block3_pool').output\n    \n    conv_6 = Conv2D(1024, (7, 7), activation='relu', padding='same', name=\"conv_6\")(pool5)\n    conv_7 = Conv2D(1024, (1, 1), activation='relu', padding='same', name=\"conv_7\")(conv_6)\n    \n    conv_8 = Conv2D(classes, (1, 1), activation='relu', padding='same', name=\"conv_8\")(pool4)\n    conv_9 = Conv2D(classes, (1, 1), activation='relu', padding='same', name=\"conv_9\")(pool3)\n    \n    deconv_7 = Conv2DTranspose(classes, kernel_size=(2,2), strides=(2,2))(conv_7)\n    add_1 = Add()([deconv_7, conv_8])\n    deconv_8 = Conv2DTranspose(classes, kernel_size=(2,2), strides=(2,2))(add_1)\n    add_2 = Add()([deconv_8, conv_9])\n    deconv_9 = Conv2DTranspose(classes, kernel_size=(8,8), strides=(8,8))(add_2)\n    \n    if fcn8 :\n        output_layer = Activation('softmax')(deconv_9)\n    elif fcn16 :\n        deconv_10 = Conv2DTranspose(classes, kernel_size=(16,16), strides=(16,16))(add_1)\n        output_layer = Activation('softmax')(deconv_10)\n    else :\n        deconv_11 = Conv2DTranspose(classes, kernel_size=(32,32), strides=(32,32))(conv_7)\n        output_layer = Activation('softmax')(deconv_11)\n    \n    model = Model(inputs=vgg.input, outputs=output_layer)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:39.143888Z","iopub.execute_input":"2021-11-04T16:50:39.144267Z","iopub.status.idle":"2021-11-04T16:50:39.161768Z","shell.execute_reply.started":"2021-11-04T16:50:39.144215Z","shell.execute_reply":"2021-11-04T16:50:39.160781Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"vgg = VGG16(include_top=False, weights='imagenet', input_shape=(width, height, 3))","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:39.163369Z","iopub.execute_input":"2021-11-04T16:50:39.164015Z","iopub.status.idle":"2021-11-04T16:50:43.069887Z","shell.execute_reply.started":"2021-11-04T16:50:39.163940Z","shell.execute_reply":"2021-11-04T16:50:43.068630Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = fcn(vgg, fcn8=True)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:43.071215Z","iopub.execute_input":"2021-11-04T16:50:43.071702Z","iopub.status.idle":"2021-11-04T16:50:43.197880Z","shell.execute_reply.started":"2021-11-04T16:50:43.071656Z","shell.execute_reply":"2021-11-04T16:50:43.197076Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:43.199174Z","iopub.execute_input":"2021-11-04T16:50:43.199643Z","iopub.status.idle":"2021-11-04T16:50:44.623600Z","shell.execute_reply.started":"2021-11-04T16:50:43.199598Z","shell.execute_reply":"2021-11-04T16:50:44.622571Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Train our model","metadata":{}},{"cell_type":"code","source":"adam = Adam(lr=0.001, decay=1e-06)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:44.625307Z","iopub.execute_input":"2021-11-04T16:50:44.625865Z","iopub.status.idle":"2021-11-04T16:50:44.671603Z","shell.execute_reply.started":"2021-11-04T16:50:44.625815Z","shell.execute_reply":"2021-11-04T16:50:44.670798Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"filepath = \"best-model-vgg.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:44.673047Z","iopub.execute_input":"2021-11-04T16:50:44.673410Z","iopub.status.idle":"2021-11-04T16:50:44.679327Z","shell.execute_reply.started":"2021-11-04T16:50:44.673363Z","shell.execute_reply":"2021-11-04T16:50:44.677136Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(train_gen, epochs=20, steps_per_epoch=num_of_training_samples//batch_size,\n                       validation_data=val_gen, validation_steps=num_of_testing_samples//batch_size,\n                       callbacks=callbacks_list, use_multiprocessing=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T16:50:44.681019Z","iopub.execute_input":"2021-11-04T16:50:44.681776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation and Vizualization","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"best-model-vgg.hdf5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history[\"val_loss\"]\nacc = history.history[\"val_accuracy\"] #accuracy\n\nplot.figure(figsize=(12, 6))\nplot.subplot(211)\nplot.title(\"Val. Loss\")\nplot.plot(loss)\nplot.xlabel(\"Epoch\")\nplot.ylabel(\"Loss\")\n\nplot.subplot(212)\nplot.title(\"Val. Accuracy\")\nplot.plot(acc)\nplot.xlabel(\"Epoch\")\nplot.ylabel(\"Accuracy\")\n\nplot.tight_layout()\nplot.savefig(\"learn.png\", dpi=150)\nplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_gen = DataGenerator(valid_folder)\nmax_show = 1\nimgs, segs = next(val_gen)\npred = model.predict(imgs)\n\nfor i in range(max_show):\n    _p = give_color_to_seg_img(np.argmax(pred[i], axis=-1))\n    _s = give_color_to_seg_img(np.argmax(segs[i], axis=-1))\n\n    predimg = cv2.addWeighted(imgs[i]/255, 0.5, _p, 0.5, 0)\n    trueimg = cv2.addWeighted(imgs[i]/255, 0.5, _s, 0.5, 0)\n    \n    plot.figure(figsize=(12,6))\n    plot.subplot(121)\n    plot.title(\"Prediction\")\n    plot.imshow(predimg)\n    plot.axis(\"off\")\n    plot.subplot(122)\n    plot.title(\"Original\")\n    plot.imshow(trueimg)\n    plot.axis(\"off\")\n    plot.tight_layout()\n    plot.savefig(\"pred_\"+str(i)+\".png\", dpi=150)\n    plot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\n* [Fully Convolutional Networks for Semantic Segmentation](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)\n* [Fully Convolutional Networks (FCN) for 2D segmentation](http://www.deeplearning.net/tutorial/fcn_2D_segm.html)\n* [Learn about Fully Convolutional Networks for semantic segmentation](https://fairyonice.github.io/Learn-about-Fully-Convolutional-Networks-for-semantic-segmentation.html)\n* [An Introduction to different Types of Convolutions in Deep Learning](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)","metadata":{}}]}